% Footer Formatting: DO NOT CHANGE -----------------------------------
% Setting the page numbers to be arabic 
\pagenumbering{arabic}

\fancyfoot{}

\fancyfoot[LE]{\colorbox{urban-footergray}{\makebox(0.2, 0.12)[r]{\fontsize{7.5}{0}\selectfont\bfseries{\MakeUppercase{}\hspace{0.2in}}}}\colorbox{urban-gold}{\makebox(0.2, 0.12)[c]{\fontsize{7.5}{0}\selectfont\bfseries{\MakeUppercase\thepage}}}\colorbox{urban-footergray}{\makebox(5.64, 0.12)[r]{\fontsize{7.5}{0}\selectfont\bfseries{\MakeUppercase{\so{\THETITLE}}\hspace{0.2in}}}}}

\fancyfoot[RO]{\colorbox{urban-footergray}{\makebox(5.64, 0.12)[l]{\fontsize{7.5}{0}\selectfont\bfseries{\hspace{0.2in}\MakeUppercase{\so{\THETITLE}}}}}\colorbox{urban-gold}{\makebox(0.2, 0.12)[c]{\fontsize{7.5}{0}\selectfont\bfseries{\MakeUppercase\thepage}}}\colorbox{urban-footergray}{\makebox(0.2, 0.12)[r]{\fontsize{7.5}{0}\selectfont\bfseries{\MakeUppercase{}\hspace{0.2in}}}}}
% Footer Formatting: DO NOT CHANGE -----------------------------------
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Report %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part{Differentially Private Methods for Validation Servers}

\section{Introduction}\label{sec:intro}

Federal tax data, derived from individuals' and businesses' tax and information returns, are invaluable resources for research on a range of topics. That research improves our understanding of individuals' and firms' responses to economic incentives. Researchers can also use the data to study areas far outside of taxation. For example, \citet{chetty2014measuring} have used tax data to study economic mobility across generations and how elementary school teacher quality affects economic outcomes later in life \citep{chetty2011does}.

However, full access to these data is available only to select government agencies, to a very limited number of researchers working in collaboration with analysts in those agencies, or through highly selective programs within the Internal Revenue Service (IRS) Statistics of Income (SOI) Division. In addition, the existing process of manually vetting each statistical release for disclosure risks is labor intensive and imperfect because it relies on subjective human review. The tremendous demand to participate in such projects, which is limited by SOI resource constraints, indicates that much more high-quality research could be conducted if a safe and less resource-intensive method were developed to expand access.

\subsection{Background on Accessing Confidential Data}\label{subsec:background}
At the IRS, the current process to release analytic results on confidential datasets requires the researcher to undergo an extensive background check (IRS clearance) to access the data, and then an IRS staff member must review any results the researcher wants to release. This process reflects the norm for researchers wishing to access federal confidential data. Either researchers gain access from a public use file that is an altered version of the confidential data or has direct access to the confidential data.

As a potential middle ground between the two extremes, the US Census Bureau provides research access to two experimental synthetic databases via the Synthetic Data Server (SDS) at Cornell University: the Synthetic Longitudinal Business Database and the SIPP Synthetic Beta Data Product \citep{benedetto2013creation,drechsler2014synthetic}. The SDS provides a validation server that allows researchers to submit their statistical programs to run on the underlying administrative data after testing it on the publicly available synthetic data.

However, the SDS has two disadvantages. First, because it is not automated, the process consumes limited staff time, which demand often exceeds. This situation causes long delays for approval. Second, reviews may be inconsistent because they are manually evaluated by humans and not adhere to formal notions of privacy that constrain the allowable output.

To address these problems, privacy researchers propose implementing a newer privacy loss definition, differential privacy (DP), as a means to automate the process \citep{dwork2006calibrating}. DP emerged from the computer science community as a rigorous definition for privacy loss associated with data publishing. Since then, many data privacy experts regard DP as the ``gold standard'' for privacy protection. It is often called a \textit{formally private} method because statisticians can mathematically prove the privacy loss that will result from a data publication that uses differentially private methods.

DP differs from prior statistical disclosure control or limitation methods because it does not require a simulated attacker or the same strong assumptions concerning how much information an intruder may have or what kind of disclosure is likely to occur. This does not imply that DP protects from all attacks, but, for a defined type of privacy loss, it offers provable amounts of protection.

At a high level, DP links the potential for privacy loss to how much the answer of a query (such as a statistic) is changed given the absence or presence of the most extreme possible person or observation in the data population. DP requires that the level of protection is set proportionally to this maximum potential change, thereby providing formal privacy protections scaled to the worst-case scenario. For further details, \citet{dwork2014algorithmic} provides a rigorous mathematical review of DP. \citet{bowen2021philosophy} covers the basics of DP and its challenges for adoption geared towards a general, mathematical audience, whereas \citet{nissim2017differential} and \citet{snoke2019differential} describe DP for a non-technical, general audience. 

In this report, we examine the feasibility of differentially private methods for our target analyses and more complex ones. Specifically, we highlight the general findings from our extensive feasibility study on several differentially private methods for releasing tabular statistics, mean and quantile statistics, and regression analyses \citep{barrientos2021}. Based on informal interviews and our tax expert collaborators, we identified these analyses as the priority analyses over others for the first stage of the validation server. There are several other analyses, such as model selection, that have been identified as important but will be explored for later development stages of the validation server. Further, we outline the methodological and practical challenges for implementing differential privacy on more complex analyses, such as regression discontinuity design.

\subsection{Background on Differential Privacy}\label{subsec:dp}
Differential privacy offers a provable and quantifiable amount of privacy protection, colloquially referred to as the privacy loss budget. Those in the data privacy and confidentiality community should note the important distinction that DP provides a statement about the algorithm (or mechanism), not the data---a common misconception. In other words, DP requires that the \textit{mechanism} or \textit{algorithm} produces an output that provably meets the definitions. We refer to these methods as differentially private algorithms or mechanisms.

In this section, we reproduce the pertinent definitions and theorems of DP with the following notation: $X\in\mathbb{R}$ is the original data set  with dimension ${n\times r}$ and $X^*$ is the private version of $X$ with dimension ${n^*\times r}$. We also define a statistical query as a function $u:\mathbb{R}^{n\times r}\rightarrow\mathbb{R}^k$, where the function maps the possible data sets of $X$ to $k$ real numbers.

\subsubsection{Definitions and Theorems}\label{subsec:def}
\begin{defn}\label{def:dp} \textbf{Differential privacy} \citep{dwork2006calibrating}:
A sanitization algorithm, $\M$, satisfies $\epsilon$-DP if for all subsets $S\subseteq Range(\M)$ and for all $X,X'$ such that $d(X,X')=1$, 
    \begin{equation}\label{eqn:dp}
        \frac{\Pr(\M( X) \in S)}{ \Pr(\M( X')\in S)}\le \exp(\epsilon)
    \end{equation}
\end{defn}
\noindent where $\epsilon>0$ is the privacy loss budget and $d(X,X')=1$ represents the possible ways that $X'$ differs from $X$ by one record.

Privacy researchers debate what this difference means. One interpretation is the presence or absence of a record, and the other has the difference as a change, where $X$ and $X'$ have the same dimensions. \citet{li2016differential} refers to these interpretations as \textit{unbounded DP} for addition or removal of a record and \textit{bounded DP} for the change of a record. They prove that unbounded DP satisfies an important composition theorem we will discuss later in this section (see theorem \ref{thm:comp}), whereas bounded DP does not. Because many differentially private methods rely on theorem \ref{thm:comp}, we use unbounded DP in our report.

Data users often criticize that $\epsilon$-DP algorithms inject too much noise into query results. Several relaxations of $\epsilon$-DP have been developed, such as approximate DP \citep{dwork2006our}, probabilistic DP \citep{machanavajjhala2008privacy}, concentrated DP \citep{dwork2016concentrated}, R\'enyi differential privacy \citep{mironov2017renyi}, and zero-concentrated DP \citep{bun2016concentrated}. Though these definitions are still formally private, they offer slightly weaker privacy guarantees. In return, they typically lessen the amount of noise required. We will cover approximate DP, also known as $(\epsilon, \delta)$-DP, R\'enyi differential privacy, and zero-concentrated in depth, because the methods we test in our study use these definitions.

\begin{defn}\label{def:adp} \textbf{$(\epsilon, \delta)$-Differential privacy} \citep{dwork2006our}:
A sanitization algorithm, $\M$, satisfies $(\epsilon, \delta)$-DP if for all $X, X'$ that are $d(X,X')=1$,
    \begin{equation}\label{eqn:adp}
        \Pr(\M( X) \in S)\le \exp(\epsilon) \Pr(\M( X')\in S) + \delta
    \end{equation}
    where $\delta\in [0,1]$. $\epsilon$-DP is a special case of $(\epsilon, \delta)$-DP when $\delta=0$.
\end{defn}

The parameter $\delta$ adds a small probability that the bound given in definition \ref{def:dp} does not hold, which can be useful when dealing with extreme yet very unlikely cases.

In 2016, \citet{dwork2016concentrated} created concentrated DP (CDP), which aimed to reduce the privacy loss over multiple computations (more on composition of multiple queries soon when discussing theorem \ref{thm:comp}).

\begin{defn}\label{def:cdp} \textbf{Concentrated differential privacy} \citep{dwork2016concentrated}:
For all data sets $(\x,\x')$ that is $d(\x,\x')=1$, a sanitization algorithm $\R$ gives $(\mu, \tau)$-concentrated differential privacy if 
\begin{equation}
    D_{\text{subG}}(\R(\x)||\R(\x')) \preceq (\mu,\tau),
\end{equation}
where $D_{\text{subG}}$ stands for \emph{subGaussian divergence}. SubGaussian divergence is two random variables $Y$ and $Z$ are $D_{\text{subG}}(Y||Z) \preceq (\mu,\tau)$ if and only if  $\mathbb{E}(L_{Y||Z})\leq \mu$ and the centered distribution of $(L_{Y||Z}-\mathbb{E}(L_{Y||Z})$ is defined and $\tau$-subgaussian, where  $L_{(Y||Z)}=\ln\left(p(Y) /p(Z) \right)$ is the privacy loss random variable.
\end{defn}

\citet{bun2016concentrated} improved concentrated DP with zero-concentrated DP (zCDP).

\begin{defn}\label{def:scdp} \textbf{Zero-concentrated differential privacy} \citep{bun2016concentrated}:
A sanitization algorithm, $\M$, satisfies $(\xi, \rho)$-zero-concentrated differential privacy if for all $X, X'$ that are $d(X,X')=1$ and $\alpha\in (1, \infty)$,
    \begin{equation}
        D_\alpha(M(X)||\M(X')\leq\xi+\rho\alpha,
    \end{equation}
    where $D_\alpha(M(X)||\M(X')$ is the $\alpha$-R\'enyi divergence (see definition \ref{def:divergence}) between the distribution of $M(X)$ and the distribution of $M(X').$
\end{defn}
\citet{bun2016concentrated} define $\rho$-zCDP to be $(0,\rho)$-zCDP. They also show in proposition 1.3 that if $M$ satisfies $\rho$-zCDP, then $M$ is $(\rho+2\sqrt{\rho\log(1/\delta)},\delta)$-DP for any $\delta>0$. For the other direction, their proposition 1.4 states that if $M$ satisfies $\epsilon$-DP, then $M$ satisfies $(1/2\epsilon^2)$-zCDP.

The next year, \citet{mironov2017renyi} introduced the idea of DP based on the concept of R\'enyi divergence. The paper defines R\'enyi divergence (definition \ref{def:divergence}) and R\'enyi DP (definition \ref{def:renyi}) as follows:

\begin{defn}\label{def:divergence} \textbf{R\'enyi divergence} \citep{renyi1961measures}: Let $P$ and $Q$ be two distributions on $X$ defined over the same probability space, and let $p$ and $q$ be their respective densities. The R\'enyi divergence of a finite order $\alpha\neq1$ between $P$ and $Q$ is defined as
    \begin{equation}
        D_\alpha(P||Q)\stackrel{\Delta}{=}\frac{1}{\alpha-1}\ln\int_X q(x)\left(\frac{p(x)}{q(x)}\right)^\alpha dx
    \end{equation}
\end{defn}

\begin{defn}\label{def:renyi} \textbf{$(\alpha, \epsilon)$-Differential privacy} \citep{mironov2017renyi}: A sanitization algorithm, $\M$, satisfies $(\alpha, \epsilon)$-DP if for all $X, X'$ that are $d(X,X')=1$,
    \begin{equation}
        D_\alpha(M(X)||\M(X')\leq\epsilon
    \end{equation}
\end{defn}

Additionally, \citet{mironov2017renyi} showed that if $\M$ satisfies $(\alpha,\epsilon)$-DP, then it satisfies $\left(\epsilon + \frac{\log(1/\delta)}{\alpha-1}, \delta \right)$-DP for all $\delta\in[0,1]$.

Many DP algorithms require repeated responses from a query system, such as a validation server. Each time a statistic or output is released, data information ``leaks'' and must be protected. DP protects the information by splitting the amount of $\epsilon$ used for each output, and the composition theorems formalize this concept.

\begin{thm}\label{thm:comp} \textbf{Composition theorems} \citep{mcsherry2009privacy,dwork2016concentrated,bun2016concentrated}:
Suppose a mechanism, $\M$, provides $(\epsilon_j$, $\delta_j)$-DP for $j=1,\ldots,k$.
  \begin{itemize}\setlength{\itemindent}{15pt}
  \item[a)] \textbf{Sequential composition:}\\
    The sequence of $\M_j(X)$ applied on the same $X$ provides $(\sum_j\epsilon_j,\sum_j\delta_j)$-DP or $(\sum_j\xi_j,\sum_j\rho_j)$-zCDP.
  \item[b)] \textbf{Parallel composition:}\\
    Let  $D_j$ be disjoint subsets of the input domain $D$. The sequence of $\M_j(X\cap D_j)$ provides $\max(\epsilon_j), \max(\delta_j)$-DP or $\max(\xi_j), \max(\rho_j)$-zCDP.
  \end{itemize}
\end{thm}

To put it more simply, suppose there are $k$ many statistical queries on $X$. The composition theorems state that the data practitioner may allocate a portion of the overall desired level of $\epsilon$ to each statistic by sequential composition. A typical appropriation is dividing $\epsilon$ equally by $k$. For example, a data practitioner might want to query the mean and standard deviation of a variable. These two queries will require using the sequential composition, allocating an equal amount of privacy budget to each query.

Conversely, parallel composition does not require splitting the budget because the noise is applied to disjoint subsets of the input domain. Privacy experts will often leverage parallel composition, for instance, to sanitize histogram counts, where the bins are disjoint subsets of the data. In this example, noise can be added to each bin independently without needing to split $\epsilon$.

The post-processing theorem is another important theorem, which states that any function applied to a differentially private output is also differentially private.

\begin{thm}\label{thm:post} \textbf{Post-Processing Theorem} \citep{dwork2006calibrating,nissim2007smooth, bun2016concentrated}:
If $\M$ be a mechanism that satisfies $\epsilon$-DP, $(\epsilon,\delta)$-DP, or $(\xi,\rho)$-zCDP, and $g$ be any function, then $g\left(\M(X)\right)$ also satisfies $\epsilon$-DP, $(\epsilon,\delta)$-DP, or $(\xi,\rho)$-zCDP.
\end{thm}

Most differentially private methods use the post-processing theorem to correct any inconsistencies or values that are not possible. Privacy researchers frequently implement post-processing to enforce structural aspects of the data, such as not releasing negative values for people's ages.

\subsubsection{Differentially Private Mechanisms}\label{subsec:mech}
In this section, we present the fundamental mechanisms that we tested that satisfy $\epsilon$-DP and $(\epsilon, \delta)$-DP algorithms. For a given value of $\epsilon$, an algorithm that satisfies DP or approximate DP will adjust the amount of noise added to the data based on the maximum possible change, given two databases that differ by one row, of the statistic or data that the data practitioner wants released. This value is commonly referred to as the global sensitivity (GS), given in Definition \ref{def:gs}.

\begin{defn}\label{def:gs} \textbf{$l_1$-Global Sensitivity} \citep{dwork2006calibrating}:
For all $X,X'$ such that $d(X,X')=1$, the global sensitivity of a function $u$ is
    \begin{equation}\label{eqn:gs}
        \Delta_1 u= \underset{d(X,X')=1}{\text{sup}} \|u(X)-u(X') \|_1 
    \end{equation}
\end{defn}

We can calculate sensitivity under different norms. For instance, $\Delta_2 u$ represents the $l_2$ norm global sensitivity, $l_2$-GS, of the function $u$. Another way of thinking about the GS is that it measures how robust the statistical query is to outliers.

While the definition is straight forward, calculating the global sensitivity can often be difficult, if not impossible. For instance, we cannot directly calculate the global sensitivity of one of the most common statistical analyses, regression, where the coefficients are unbounded. To address this issue, privacy researchers had to be creative in figuring out ways to add noise to regression analyses.

The most basic mechanism satisfying $\epsilon$-DP is the Laplace Mechanism, given in Definition \ref{def:lap}, first introduced by \citet{dwork2006calibrating}.

\begin{defn}\textbf{Laplace Mechanism} \citep{dwork2006calibrating}: \label{def:lap}
The Laplace Mechanism satisfies $\epsilon$-DP by adding noise to $u$ that are drawn from a Laplace distribution with the location parameter at 0 and scale parameter of $\Delta_u\epsilon^{-1}$ such that 
    \begin{equation}\label{eqn:lap}
        u^\ast(X)=u(X)+Laplace\left(0,\Delta_1 u \epsilon^{-1}\right)
    \end{equation}
\end{defn}

Another popular mechanism is the Gaussian Mechanism that satisfies $(\epsilon, \delta)$-DP, given in Definition \ref{def:gauss}, which uses the $l_2$-GS of the statistical query.

\begin{defn}\label{def:gauss} \textbf{Gaussian Mechanism} \citep{dwork2014algorithmic}:
    The Gaussian Mechanism satisfies $(\epsilon,\delta)$-DP by adding Gaussian noise with zero mean and variance, $\sigma^2$, such that
        \begin{equation}\label{eqn:gauss}
            u^\ast(X)=u(X)+N\left(0, \sigma^2 I \right)
        \end{equation}
    where $\sigma=\Delta_2 u \epsilon^{-1} \sqrt{2 \log(1.25/\delta)}$. 
\end{defn}

Although \citet{dwork2014algorithmic} proposed Gaussian Mechanism for $(\epsilon,\delta)$-DP, the Gaussian Mechanism satisfies $(\Delta^2_2/2\sigma^2)$-zCDP (Per Proposition 1.6 from \citet{bun2016concentrated}). Additionally, if the $l_2$-GS is 1 (which is true for all counting queries), then the Gaussian mechanism satisfies $\left(\alpha, \frac{\alpha}{2\sigma^2}\right)$-R\'enyi DP (Corollary 3 from  \citet{mironov2017renyi}). Privacy researchers use this relationship for multiple counting queries to reduce the amount of noise being added from the Gaussian mechanism. More specifically, $\left(\alpha, \frac{m\alpha}{2\sigma^2}\right)$-R\'enyi DP is equivalent to $\left(m\frac{\alpha}{2\sigma^2}+\frac{\log(1/\delta)}{\alpha-1}, \delta\right)$-DP \citep{mironov2017renyi,wang2019subsampled}.

Both the Laplace and Gaussian Mechanisms are simple and quick to implement, but only apply to numerical values (without additional post-processing, Theorem \ref{thm:post}). A more general $\epsilon$-DP mechanism is the Exponential Mechanism, given in Definition \ref{def:exp}, which allows for the sampling of values from a noisy distribution rather than adding noise directly. Although the Exponential Mechanism can apply to any type of statistic, many theoretical algorithms using the Exponential Mechanism are computationally infeasible for practical applications without modifications, such as limiting the possible outputs for a particular statistic, $\theta$, on $X$.

\begin{defn}\label{def:exp} \textbf{Exponential Mechanism} \citep{mcsherry2007mechanism}:
    The Exponential mechanism releases values with a probability proportional to
        \begin{equation}\label{eqn:exp}
            \exp \left(\frac{\epsilon u(X, \theta)}{2\Delta_1 u}\right)
        \end{equation}
    and satisfies $\epsilon$-DP, where $u(X,\theta)$ is the score or quality function that determines the values for each possible output, $\theta$, on $X$.
\end{defn}

\subsection{Tax Data Use Cases}\label{subsec:data}
When deciding the scope of the statistical analyses the validation server will cover, we interviewed six economists with focuses on econometrics and causal inference, taxation, and fields that use tax data. They emphasized the importance of custom tabulations for their work, which one economist referred to as ``low-hanging fruit.'' Many recent journal articles using tax data relied on aggregated data. Examples include the ongoing analyses of income inequality \citep{auten2018income} and wealth inequality \citep{smith2019top}, the growth of gig work in the economy \citep{collins2019gig}, and decisions on retirement savings \citep{brady2020reconciling}.

Other researchers have used aggregations of data in linear regressions on topics such as the effects of attitudes towards government and tax evasion \citep{cullen2021political}, the effect of tax credits on the labor market \citep{tong2014impact} and optimal taxation \citep{piketty2014optimal}. Researchers have also used aggregated data to infer behavioral responses by examining bunching at kink points \citep{chetty2011adjustment} and notches \citep{kleven2013using} in the tax code. \citet{mortenson2020bunching}, for example, investigate bunching at kink points formed from the formula for computing earned income tax credits.

With these analyses in mind, we tested the differentially private methods on the IRS SOI Public Use File (PUF) \citep{barrientos2021}. This is a database of sampled individual income tax returns with privacy-protections applied that is developed and released annually by IRS SOI. Several organizations, such as the American Enterprise Institute, the Urban-Brookings Tax Policy Center, and the National Bureau of Economic Research develop PUF-based microsimulation models that help inform the public on potential impacts of policy proposals. But, access to this public file is limited to certain institutions, and we cannot provide the full data. For this reason, we also test on the 2020 Current Population Survey Annual Social and Economic Supplement (CPS ASEC), which is a publicly available dataset. Crucially, it has similar variables as the PUF, and the case-study results on the CPS will be similar to those run on the IRS SOI.

A tax expert would likely query the following as exploratory data analysis before statistical modeling, which we based our feasibility study on.

\begin{itemize}
    \item \textbf{Single counting query} - ``How many tax returns have salary and wage income in excess of \$100,000?''
    \item \textbf{Mean statistic query} - ``What are the means for the total population and subsets of the total population?''
    \item \textbf{Quantile statistic query} - ``What is the income threshold for the top 10\% of earners?''
    \item \textbf{Regression query} - ``What is first dollar marginal tax rates?''
\end{itemize}

\section{Tested Differentially Private Algorithms}\label{sec:dp-mech}
In this section, we survey differentially private methods we explored for the feasibility study and highlight the general results.

\subsection{Tabular Statistics}
Researchers have fully developed differentially private tabular statistics. The literature already shows that adding Laplace noise produces the most accurate estimates for single tabular queries. For example, \citet{rinott2018confidentiality} found for disseminating frequency tables, Laplace mechanism performed the best overall. Other DP research reports similar results, because the Laplace mechanism is still ``hard to beat'' when the data has a large number of observations or there are a lot of parameters and attributes to consider \citep{bowen2021differentially,shlomo2018statistical,liu2018generalized}.

Although the Gaussian mechanism produces wider tails, adding Gaussian noise will perform better than adding Laplace noise for multiple counting queries due to how the tails compose \citet{wang2019subsampled}. We can take advantage of this composition property by leveraging the R\'enyi differential privacy, which reduces the amount of noise added to each count. A drawback to using the Gaussian mechanism is that it satisfies $(\epsilon,\delta)$-DP, requiring the data user to balance two privacy parameters instead of one.

There are other differentially private algorithms that tackle more complex tabular statistics. However, the tax analyses we selected for the feasibility study are simple counting queries. This means the Laplace and Gaussian mechanisms will outperform other methods. Therefore, we do not need to explore the more extensive differentially private tabular algorithms or apply any sophisticated post-processing.

This brings up the question of why even test the Laplace and Gaussian mechanisms for our study. Even though there is extensive literature, we still test the Laplace and Gaussian mechanisms for two reasons. First is to verify these methods work well on our tax data. Second is the feasibility study can provide additional context for the privacy-utility trade-off in deciding what is an appropriate privacy loss budget.

In \citet{barrientos2021}, we confirmed that the Laplace mechanism outperformed the Gaussian mechanism, even for larger values of $\delta$. We note that both methods are unbiased, and for $\epsilon \geq 5$ they both perform well enough that there is little difference between them. Overall, these results suggest the Laplace mechanism would perform well to enable researchers to query private histograms.

\subsection{Quantile Statistics}
The leading methods for generating differentially private quantiles use either the Laplace mechanism or the Exponential mechanism. For instance, \citet{smith2011privacy} proposed an algorithm, \textit{IndExp}, for selecting individual quantiles using the Exponential mechanism. \textit{IndExp} has since been implemented in the SmartNoise\footnote{SmartNoise DP library, accessed July 20, 2021. https://github.com/opendp/smartnoise-core/tree/develop/whitepapers/mechanisms/exponential\_median} and IBM\footnote{IMB DP library, accessed July 20, 2021. https://github.com/IBM/differential-privacy-library/blob/main/diffprivlib/tools/quantiles.py} DP libraries. This method was recently extended by \citet{gillenwater2021differentially} to two other algorithms, \textit{AppIndExp} and \textit{JointExp}. The former is the same as \textit{IndExp} but uses the composition theorem from \citet{dong2020optimal} to choose an optimal $\epsilon$ for multiple queries given a total ($\epsilon$, $\delta$), and the latter samples multiple quantiles jointly. Using the Laplace mechanism, \citet{nissim2007smooth} developed an approach for sampling median values using smooth sensitivity that can be extended to query any other quantile. We will refer to this method as \textit{Smooth} from now on. A few other approaches or adaptations of the approaches listed here have been proposed, but we did not test them because they require fine tuning based on distribution assumptions that a researcher might not realistically have in our validation server setting.

In \citet{barrientos2021}, we found overall that both \textit{AppIndExp} and \textit{JointExp} offer high quality quantiles, and they were generally preferable to \textit{Smooth} for all but the highest values of $\epsilon$. \textit{JointExp} performs better at estimating the zero-valued quantiles, though \textit{AppIndExp} offers high quality performance at most levels of $\epsilon$ and $\delta$. Similarly, \textit{JointExp} has slightly lower relative bias on average for the non-zero quantiles, but the difference between the two algorithms is very small. 

For a practical validation server implementation, \textit{AppIndExp} and \textit{JointExp} both appear sufficient to return accurate quantile estimates. The choose would likely depend on whether the system deploys pure DP or approximate DP. We do not recommend that a system utilize \textit{Smooth} unless queries are made with very high levels of $\epsilon$. Additionally, we find that \textit{AppIndExp} returns more equally biased results for each quantile, since they are drawn independently. On the other hand, \textit{JointExp} can return biased results, which are more biased for some quantiles than others.

In our application, the quantiles following an exponential trend. \textit{JointExp} returns more accurate results for the lowest and highest quantiles while returning less accurate results for those in between. It may be preferable to choose one or the other algorithm depending on the application.

\subsection{Means and Confidence Intervals}
For mean estimates, we specifically reviewed differentially private methods that released means with their associated confidence intervals (CI). When we explored the general literature, we found that common approaches use the Laplace mechanism, Gaussian mechanism, or Exponential mechanisms for releasing some means with CIs.

As with the quantiles, some methods require more information than is realistic for our application. For instance, \citet{karwa2017finite, bowen2020comparative, d2015differential, biswas2020coinpress} require the researcher to set bounds for the standard deviation to calculate the global sensitivity to add Laplace noise. In a validation server setting, a researcher might not have a good sense of the bounds. Given this limitation, we did not test them for our study.

\citet{du2020differentially} conducted a comprehensive research study that aimed to move the theory to practice for releasing differentially private CIs. The authors developed five new methods, two based on directly applying Laplace noise named \textit{NOISYVAR} and \textit{NOISYMAD} and three based on querying quantiles from the Exponential mechanism  to estimate the standard deviation named \textit{CENQ}, \textit{SYMQ}, and \textit{MOD}. \citet{du2020differentially} also compared their methods against \citet{karwa2017finite}, \citet{d2015differential}, and \citet{brawner2018bootstrap}. They found the two best performing for simulated Gaussian data were \textit{NOISYMAD} and \textit{SYMQ}. \textit{NOISYMAD} adds Laplace noise to the mean absolute deviation and the associated standard deviation, whereas \textit{SYMQ} computes to different quantiles that are equal distance away from the median and adds noise from the Exponential mechanism. A potential limitation to the approach using quantiles is that it strongly assumes the data are approximately Gaussian. We chose not to test these methods on heavily skewed data because preliminary tests showed they provided quite poor performance.

In \citet{barrientos2021}, the results show that all methods are approximately unbiased. \textit{NOISYMAD} and \textit{NOISYVAR} perform similarly and both provide highly accurate statistics. BHM performs comparably to the other two methods only when $\delta = 0.01$ and when $\epsilon < 1$. Otherwise, the other two clearly outperform BHM.

For the associated confidence intervals, we find \textit{NOISYVAR} offers the best performance at all but the lowest level of $\epsilon$. \textit{NOISYMAD} performs well for very small values of $\epsilon$, but, as $\epsilon$ gets larger, the width of the CIs produced by \textit{NOISYMAD} shrinks and are consistently more narrow than the confidential CI. These results would produce overly confident inference for researchers making this query. Overall, \textit{NOISYVAR} is preferable, and it is capable of providing sufficiently accurate confidence intervals for higher levels of $\epsilon$.

\subsection{Regression Analyses}
In this section, we begin with an overview of the currently available differentially private approaches for regression analysis. We then explain the criteria for including methods in the feasibility study, where we discuss the selected methods and any adaptions made before inclusion in more detail.

\subsubsection{Traditional Differentially Private Approaches for Regression Analyses}\label{subsubsec:reg_rev}

Differentially private methods for regression can be classified according to the outputs they produce: i) point estimates only; ii) point estimates and uncertainty estimates; and  iii) other outputs. Because we are particularly interested in methods that provide more comprehensive inferences, we focus our study on methods that output both point and uncertainty estimates. For interested readers, \citet{barrientos2021} has a brief description of the other two types of methods.

For providing both point and uncertainty estimates, \citet{sheffet2017differentially} developed $(\epsilon,\delta)$-differential private algorithms that, with certain probability, output summary statistics useful to perform either traditional linear regression or ridge regression. When the outputs are summaries for ridge regression, the penalization parameter is a function of the algorithm's inputs instead of predefined by the user. \citet{sheffet2017differentially} derives CIs and t-statistics that account for the noise added to the confidential summaries and shows how such statistics relate to the underlying truth. In addition, the author discusses a different algorithm that adds Gaussian noise directly to the sufficient statistics and shows that users can possibly obtain CIs for the regression coefficients under certain conditions (e.g., the norm of the true regression coefficients is upper bounded). Despite the promise of this method, we do not include it in our study due to barriers to implementation.

In the spectrum of methods that add noise to the sufficient statistic for normal linear models, \citet{sheffet2019old} proposed a $(\epsilon,\delta)$-differentially private mechanism that draws the random noise from the Wishart distribution. The mechanism defines a noisy statistic that preserves the property of being positive-definite. \citet{wang2019differentially} also propose $(\epsilon,\delta)$- and $\epsilon$-differentially private methods that release noisy versions of the summary statistic while preserving positive definiteness. These methods add noise using either a normal distribution (for $(\epsilon,\delta)$-differentially privacy) or the spherical analogue of the Laplace distribution (for $\epsilon$-differential privacy). The positive definiteness is achieved by using eigenvalue decomposition and censoring the eigenvalues falling below a given threshold. Although \citet{sheffet2019old} and \citet{wang2019differentially} do not derive CIs or t-values under the proposed mechanisms for the normal linear model, these contributions pave the way to develop differentially private methods that allow full inference for regression coefficients.  

\citet{ferrando2020general} developed a general approach to produce point and interval estimates in different scenarios, including linear regression. The authors outline two $\epsilon$-differentially private strategies, both employing a noisy version of the sufficient statistic. The first one applies the noisy statistics in classic OLS point and interval estimators. The accuracy and coverage of this strategy is ensured by large-sample arguments. The second strategy also uses plug-in estimators, but computes CIs by means of parametric bootstrap. The method accounts for the injected noise and the underlying sampling distribution. A major drawback of these two approaches is that it is not clear how to compute points and interval estimates of the coefficients when the inverse of the noisy matrix is not positive-definite. But, we employ this method as part of the feasibility study.

The approach described \citet{bernstein2019differentially} also relies on a noisy version of the sufficient statistics. The procedure uses a Bayesian framework and employs a large-sample distributional characterization of the sufficient statistics. Using a Bayesian approach allows the authors to draw from the posterior distribution of the regression coefficient and, thus, provide point and interval estimates. Another method that draws from the posterior distribution of the regression coefficients is \citet{wang2018revisiting}. However, under this approach, users need to spend part of the privacy budget for each draw. This particular aspect limits the applicability of \citet{wang2018revisiting}'s algorithm since any accurate Monte-Carlo approximations would require to divide the privacy budget into too many small values. Dividing the privacy parameter too much will dramatically decrease the statistical usefulness of the method.

When narrowing down methods for the feasibility study, we only consider frequentist approaches. Therefore, we do not include \citet{bernstein2019differentially} and \citet{wang2018revisiting}'s approaches in this iteration of the validation server. Nonetheless, we plan to consider Bayesian approaches in future versions of the validation server.

\subsubsection{Criteria for Method Inclusion}

When selecting methods to include in the study, we sought to maximize the statistical usefulness of the outputs and the feasibility of implementation. We select methods that are frequentist, that can be used for linear regression models with normal errors, and that can handle multiple predictors. We restrict our selection to methods that provide a full inference. For example, we exclude methods that only provide t-values for regression coefficients. Finally, we exclude procedures that meet the criteria above yet are hard to implement. Thus, we omit instances when: 
\begin{itemize}
    \item a manuscript lacks the information needed to implement the method it describes
    \item a pseudo-code is absent and implementation requires a deep understanding of the theory supporting the proposed method,
    \item the method has difficult to fix errors,
    \item the authors failed to reply to inquiries about their method or its implementation, or
    \item the method achieves differential privacy under no testable assumptions.
\end{itemize}

\subsubsection{Selected Methods and Adaptations}\label{subsubsec:reg_meth}
Only one method, \citet{ferrando2020general}, met all of the inclusion criteria. But, we also included \citet{brawner2018bootstrap}'s method, because, even though it was not originally designed to for linear regression, a small adaptation of the approach would make it eligible. We modified both \citet{ferrando2020general}'s and \citet{brawner2018bootstrap}'s method and obtained new versions of both methods to compare in the feasibility study. 

Specifically, \citet{ferrando2020general}'s approach employs a parametric bootstrap to approximate the distribution of the coefficients' estimator while accounting for both the underlying data-generating distribution and the differentially private mechanism (see Algorithm 3 in \citet{ferrando2020general} for the method's implementation). Although the original method uses the Laplace mechanism to achieve differential privacy, it can also employ other mechanisms after simple adaptations. For that reason, we adapted Algorithm 3 to compare the method's performance using the Analytic Gaussian mechanism \citet{balle2018improving}, the mechanism in Algorithm 2 from \citet{sheffet2019old} (hereafter, the Wishart mechanism), and the mechanisms in Algorithm 2 from \citet{wang2019differentially} (hereafter, the Normal and Spherical Laplace mechanisms). A full discussion on each of the methods and their adaptations can be found in \citet{barrientos2021}.

\tbf{[\MakeUppercase{insert regression analyses result details}]}

\section{Conclusion}\label{sec:conclusion}
We believe the feasibility study is the first comprehensive evaluation of various differentially private methods for practical applications within a validation server framework for real-world data \citep{barrientos2021}. We found that differentially private algorithms for summary statistics performed moderately well if the privacy loss budget is larger than 1, whereas methods for regression analyses still need improvement. Based on conducting the study and reviewing the results, we identified the methodological and practical challenges for implementing differential privacy on more complex analyses and avenues for future work.

\subsection{Challenges for More Complex Analyses}
We found that existing differentially private regression analyses were limited in applicability and often added more noise than needed to protect privacy. For example, the tested differentially private methods added multiplicative rather than additive noise, resulting in estimates that were often so noisy that they would be of little practical use. Additionally, we did not test many other differentially private regression analyses, because they did not meet our inclusion criteria, such as not reporting standard errors or applying to only normally distributed data. Similar to some of the mean and quantile methods, more research is needed to develop methods that are robust to other data types.

Besides the methodological issues, we encountered challenges with coding the various differentially private methods. We sometimes discovered errors in pseudo code from a manuscript. Other times we found bugs from code we collected from GitHub code or from the author(s). Overall, these are minor issues, and we informed the author(s) of the bugs we found.

However, author(s) that created their code for a particular application was more problematic. While we do not expect the privacy researchers to provide production ready code, we often discovered the research code to be messy, hard to read, and difficult to alter for our use cases. This situation is still preferable to not having any code or not being able to create the code based on the paper. We encountered this problem a few times, which prevented us from implementing some approaches. We reached out to author(s) when no code was available. If the author(s) did not respond, we attempted to code the methods ourselves. But, in some cases, the manuscript didn't provide enough information for us to implement the method and was thus excluded from the feasibility study.

These issues emphasize the importance of releasing open-source code to facilitate wider use and acceptance differentially private algorithms in practice. Recognizing this need, OpenDP from Harvard University and SmartNoise from Microsoft are developing a suit of open-source software tools to implement differentially private methods.\footnote{To learn more about OpenDP and SmartNoise, check out their websites at https://opendp.org/ and https://smartnoise.org/, respectively.} These platforms are still under development, which is why we did not utilize their code for the feasibility study.

\subsection{Avenues for Future Work}\label{subsec:future}
We identified a few areas for future work based on the feasibility study results and consulting with colleagues in the field. One area for improvement is testing differentially private algorithms on data that are not Gaussian. Many of the methods we tested performed well in their respective papers, because the authors tested on well-behaved or normally distributed data. Real-world data are often skewed, such as the 2012 SOI PUF and CPS ASEC data, which resulted in these same methods performing poorly.

Another area is developing differentially private algorithms with a focus on statistical inferences. Many data privacy experts create differentially private regression analyses to output accurate predictions, such as classification. But, these methods performed poorly on reporting accurate enough statistical inferences, or even lacked reporting the standard error, for our use cases. This appears to be a significant void in the data privacy and confidentiality literature that must be addressed.

With these potential research areas in mind, our next steps will be to development new privacy-preserving methodologies driven by tax policy use cases and how to handle survey weights. We would focus on improving formal privacy-preserving methodology for ordinary least squares regression coefficient estimates and then expand that research to address other important economic use cases. Tax experts indicated that regression discontinuity and regression kink designs are essential for much of their research using annual tax data. We are not aware of any research on formal methods to protect privacy in regression discontinuity and regression kink designs.

When developing new differentially private algorithms, we will examine both relaxed differential privacy definitions and alternative privacy definitions. For relaxed definitions, we will focus our efforts on the differentially private methods that used the $(\epsilon,\delta)$-DP and zCDP definitions, because methods that satisfied those definitions had the highest accuracy in the feasibility study. For possible alternative privacy definitions, a promising area is to compute the sensitivity of an estimate based on the observed dataset instead of the possible population data. This sensitivity, known as the local sensitivity, is defined as the maximum change in a statistic from adding or removing a single record within the observed data rather than the superset of possible datasets or population data \citep{nissim2007smooth}. Under this framework, we can calculate the bounds of the outputs of the statistical analyses more easily than the other differentially private relaxations that rely on calculating the global sensitivity.

However, the drawback for the alternative definition is that methods under this framework are no longer formally private because we use local sensitivity instead of global sensitivity. In other words, because the definition depends on the observed data, we cannot quantify the privacy loss under the same assumptions as DP---the absolute worst case scenario. Also, we lose the ability to automate the system more easily to prevent certain privacy attacks. For instance, validation server users might try to ``game the system.'' This is difficult to predict. An automated system using a differentially private framework can already handle this scenario, because DP must consider all the ways a user might leak private information.

If we use an alternative definition, we would need to define a threat model to provide context for the potential privacy loss and possible ways people will try to ``game the system.'' This approach could be considered reverting back to traditional statistical disclosure control privacy definitions that tend to be ad-hoc. But, many in the data user community have criticized DP (and other DP relaxations) as being too conservative in assuming the absolute worst case scenario. Exploring these alternative definitions with a threat model could be a possible way to bridge traditional statistical disclosure control privacy definitions and DP.

Another area for future work is determining what is an appropriate privacy loss budget for the validation server. Deciding the total privacy loss budget involves many factors that requires careful consideration and research, such as total number of queries. Within this research area, we could explore whether we should advise users on how to ``optimize'' their privacy loss budget, such as via the global sensitivity or the associated standard error.

We must also consider other social factors beyond limiting the total number of queries. For example, suppose User A queries a statistic and a couple months later User B queries the same statistic. How should the validation server handle this scenario? There are two general options: i) treat both queries A and B as separate queries and charge both users their respective privacy loss budgets; or ii) use the privacy preserving result from A for B. There are pros and cons to both scenarios.

The former avoids the conflict of notifying User B that User A has already conducted the analyses. Sometimes researchers do not want others to know what kind of analyses they are conducting for various reasons, such as avoid being scooped.\footnote{Scooped refers to the instance, where User B uses the ideas of User A, but publishes before User A.} But, submitting the queries separately would result in User A and User B having slightly different answers to the same statistic. In addition to other educational materials, we would need to clearly and carefully inform validation server users that both answers are valid.

For the latter scenario, the users could benefit from reducing the privacy loss cost. User A and User B could split the cost evenly, preserving more of their individual privacy loss budget. Additionally, User A and User B would have the same statistical result, removing the necessity to carefully explain why the same analyses produces different results when submitted by two different users.

Additionally, we should explore the impact and outcome of a user publishing their privacy-preserving results in a peer-reviewed journal. Some journals require users to provide open-source code and data when possible. To meet these requirements, the validation server would assign an identification number to every query submitted to the system. The journal could then contact the validation server managers to verify whether the user conducted the particular analysis. We plan to partner with more tax experts to see if privacy-preserving results from our validation server would be accepted in a peer-reviewed journal.

In future stages of the validation server project, we will need to develop a privacy-preserving model selection for the regression analyses. A good model selector would ideally prevent the user submitting several queries to identify the ``best'' model, preserving more of a user's privacy loss budget. Another issue we must tackle soon is how to handle error messages or privacy violations that could lead to a ``server crash.'' Specifically, a user could submit queries on a subset that has too few observations or is empty. The validation server would then send an error message that would indirectly inform the user additional information, such as there are few or no observations in the subset. Given this possibility, we would also need to limit multiple queries on subsampling the data for various analyses to avoid a user trying to ``game'' the system and discover more information. A potential solution is to limit queries that only work on the synthetic data, which will likely reduce a significant number of possible error messages or server crashes. More research is needed to determine how much the synthetic data could avoid these problems on the validation server.

We hope that this study provides the data privacy and confidentiality community with a better understanding the capabilities, limitations, and challenges of current differentially private methods for summary statistics and regression analyses. 